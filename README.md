# Data Engineering Zoomcamp Project

The aim of the project was to create a data engineering project that would cover the [Data Engineering Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp) course with its scope and technologies. This is the final project concerning this course. This project is about the [uk_crimes_2023](https://www.kaggle.com/datasets/marshuu/crimes-in-uk-2023?resource=download&select=2023-01-northamptonshire-street.csv) dataset, which represents crime data in the UK in January 2023. The project is based on batch processing.

## Description

Steps taken in the project:

* Selection a dataset that I'm interested in
* Creation a pipeline for processing this dataset and putting it to a datalake
* Creation a pipeline for moving the data from the lake to a data warehouse
* Transform the data in the data warehouse: prepare it for the dashboard
* Create a dashboard

## Technologies

* Cloud: GCP
* Infrastructure as code (IaC): Terraform
* Data Lake: Google Cloud Storage
* Workflow orchestration: Prefect
* Data Wareshouse: BigQuery
* Batch processing: dbt
* Dashboard: Looker Studio

## Getting Started

### Dependencies

* Linux-Ubuntu 20.04
* Python 3.9
* Terraform v1.4.0
* Google Cloud SDK 421.0.0
* Anaconda 2022.10

### Installing

#### The first thing to do is to clone the repository:

```sh
$ git clone https://github.com/bartoszsklodowski/stock_values_NN.git
$ cd stock_values_NN
```

#### Create a virtual environment to install dependencies in and activate it:

* Default venv:
    ```sh
    $ python3 -m venv .venv
    $ source .venv/bin/activate
    ```
* Conda environment:

    ```bash
    conda create -n environment_name python=3.9
    ```

#### Then install the dependencies:

```sh
(.venv)$ pip install -r requirements.txt
```
Note the `(.venv)` in front of the prompt. This indicates that this terminal
session operates in a virtual environment set up by `virtualenv` or `conda`.


#### Google Cloud Platform setup:

* Create an account on GCP
* Create gcp project
* Setup a service account for this project and download the JSON authentication key files
    * Permissios to set:
        * BigQuery Admin
        * Storage Admin
    * Enable these APIs for your project:
        * https://console.cloud.google.com/apis/library/iam.googleapis.com
        * https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com
    * Set the environment variable to point to the auth keys:
        ```bash
        export GOOGLE_APPLICATION_CREDENTIALS="<path/to/authkeys>.json"
        ```
    * Authenticate application using gcloud CLI: 
        ```bash
        gcloud auth application-default login
        ```

Run Terraform commands to set cloud assets in folder `./infrastructure` and provide the required information

```bash
terraform init
terraform plan
terraform apply
```

#### Workflow orchestration

#### Kaggle

* create Kaggle account
* create API Token and put `kaggle.json` to a directory: `./home/.kaggle`

#### Prefect
Set gcp information in `make_gcp_blocks.py` file and file paths in others python scripts.

Run python scripts:

```bash
prefect start orion
python blocks/make_gcp_blocks.py
python flows/etl_web_to_gcs.py
python flows/etl_gcs_to_bq.py
```
#### Dbt transformations:

Complete the information about your project in the file dbt_transformation/uk_crimes/profiles.yml

Run dbt commands to make transformations:

```bash
dbt deps
dbt seed
dbt run
dbt test
```

or use build command:

```bash
dbt build
```

Now you can build your own reports using Looker studio or look at [mine](https://lookerstudio.google.com/reporting/cd17fc94-38e8-41f8-a5fc-c38ce8a34c18)


## Author

Bartosz Sk≈Çodowski 
